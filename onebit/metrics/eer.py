import torch
import numpy as np
from typing import Tuple


def calculate_eer(scores: torch.Tensor, labels: torch.Tensor) -> Tuple[float, float]:
    scores_np = scores.detach().cpu().numpy()
    labels_np = labels.detach().cpu().numpy()
    
    sorted_indices = np.argsort(scores_np)
    sorted_scores = scores_np[sorted_indices]
    sorted_labels = labels_np[sorted_indices]
    
    n_bonafide = np.sum(labels_np)
    n_spoof = len(labels_np) - n_bonafide
    
    if n_bonafide == 0 or n_spoof == 0:
        return 0.0, 0.0
    
    false_accept_rate = np.concatenate([[1.0], (n_spoof - np.cumsum(sorted_labels == 0)) / n_spoof])
    false_reject_rate = np.concatenate([[0.0], np.cumsum(sorted_labels == 1) / n_bonafide])
    
    eer_idx = np.argmin(np.abs(false_accept_rate - false_reject_rate))
    eer = (false_accept_rate[eer_idx] + false_reject_rate[eer_idx]) / 2.0
    
    THRESHOLD_EPSILON = 1e-6
    if eer_idx == 0:
        threshold = sorted_scores[0] - THRESHOLD_EPSILON
    elif eer_idx == len(sorted_scores):
        threshold = sorted_scores[-1] + THRESHOLD_EPSILON
    else:
        threshold = sorted_scores[eer_idx - 1]
    
    return float(eer), float(threshold)

# Tests generated by claude(cursor)
if __name__ == '__main__':
    # Test EER calculation with various scenarios
    print("Testing EER calculation...")
    
    # Test case 1: Simple case with known distribution
    print("\n1. Testing simple case:")
    scores1 = torch.tensor([0.1, 0.3, 0.5, 0.7, 0.9])
    labels1 = torch.tensor([0, 0, 1, 1, 1])  # 0 = spoof, 1 = bonafide
    eer1, threshold1 = calculate_eer(scores1, labels1)
    print(f"   Scores: {scores1.tolist()}")
    print(f"   Labels: {labels1.tolist()}")
    print(f"   EER: {eer1:.4f}, Threshold: {threshold1:.4f}")
    
    # Test case 2: More realistic scenario with random data
    print("\n2. Testing with random data:")
    torch.manual_seed(42)  # For reproducible results
    scores2 = torch.rand(100)
    labels2 = torch.randint(0, 2, (100,))
    eer2, threshold2 = calculate_eer(scores2, labels2)
    print(f"   100 random samples")
    print(f"   Bonafide count: {labels2.sum().item()}")
    print(f"   Spoof count: {(labels2 == 0).sum().item()}")
    print(f"   EER: {eer2:.4f}, Threshold: {threshold2:.4f}")
    
    # Test case 3: Perfect separation
    print("\n3. Testing perfect separation:")
    scores3 = torch.tensor([0.1, 0.2, 0.3, 0.7, 0.8, 0.9])
    labels3 = torch.tensor([0, 0, 0, 1, 1, 1])
    eer3, threshold3 = calculate_eer(scores3, labels3)
    print(f"   Scores: {scores3.tolist()}")
    print(f"   Labels: {labels3.tolist()}")
    print(f"   EER: {eer3:.4f}, Threshold: {threshold3:.4f}")
    
    # Test case 4: Edge case - all bonafide
    print("\n4. Testing edge case - all bonafide:")
    scores4 = torch.tensor([0.1, 0.5, 0.9])
    labels4 = torch.tensor([1, 1, 1])
    eer4, threshold4 = calculate_eer(scores4, labels4)
    print(f"   Scores: {scores4.tolist()}")
    print(f"   Labels: {labels4.tolist()}")
    print(f"   EER: {eer4:.4f}, Threshold: {threshold4:.4f}")
    
    # Test case 5: Edge case - all spoof
    print("\n5. Testing edge case - all spoof:")
    scores5 = torch.tensor([0.1, 0.5, 0.9])
    labels5 = torch.tensor([0, 0, 0])
    eer5, threshold5 = calculate_eer(scores5, labels5)
    print(f"   Scores: {scores5.tolist()}")
    print(f"   Labels: {labels5.tolist()}")
    print(f"   EER: {eer5:.4f}, Threshold: {threshold5:.4f}")
    
    # Test case 6: Large dataset simulation
    print("\n6. Testing large dataset:")
    torch.manual_seed(123)
    # Generate bonafide scores (higher values)
    bonafide_scores = torch.normal(0.7, 0.2, (500,)).clamp(0, 1)
    # Generate spoof scores (lower values)
    spoof_scores = torch.normal(0.3, 0.2, (500,)).clamp(0, 1)
    
    scores6 = torch.cat([bonafide_scores, spoof_scores])
    labels6 = torch.cat([torch.ones(500), torch.zeros(500)])
    
    # Shuffle the data
    perm = torch.randperm(1000)
    scores6 = scores6[perm]
    labels6 = labels6[perm]
    
    eer6, threshold6 = calculate_eer(scores6, labels6)
    print(f"   1000 samples (500 bonafide, 500 spoof)")
    print(f"   Bonafide mean score: {scores6[labels6 == 1].mean():.4f}")
    print(f"   Spoof mean score: {scores6[labels6 == 0].mean():.4f}")
    print(f"   EER: {eer6:.4f}, Threshold: {threshold6:.4f}")
    
    print("\nAll tests completed!")
    
    # Validation: Test that the threshold actually gives approximately equal FAR and FRR
    print("\n7. Validating threshold for case 6:")
    predictions = (scores6 >= threshold6).float()
    far = ((predictions == 1) & (labels6 == 0)).sum().float() / (labels6 == 0).sum().float()
    frr = ((predictions == 0) & (labels6 == 1)).sum().float() / (labels6 == 1).sum().float()
    print(f"   At threshold {threshold6:.4f}:")
    print(f"   False Accept Rate: {far:.4f}")
    print(f"   False Reject Rate: {frr:.4f}")
    print(f"   Difference: {abs(far - frr):.4f}")